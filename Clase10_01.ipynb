{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase10-01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpF9hoqQgQagbHO3Ke6euh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PabloCGarcia/bioDeepLearning/blob/main/Clase10_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY9isCLX1Qca"
      },
      "source": [
        "Clase de introducciÃ³n de deepchem, experimentos con deep learning con datasets conocidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEFWsZon0nhZ"
      },
      "source": [
        "#!curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\r\n",
        "#import conda_installer\r\n",
        "#conda_installer.install()\r\n",
        "#!/root/miniconda/bin/conda info -e\r\n",
        "!pip install -q condacolab\r\n",
        "import condacolab\r\n",
        "condacolab.install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3p9WzwBIoW_"
      },
      "source": [
        "import condacolab\r\n",
        "condacolab.check()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYQmQqejIrcb"
      },
      "source": [
        "!conda install -y -c rdkit rdkit==2020.09.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpeli_r0IwW3"
      },
      "source": [
        "!conda install -y -c conda-forge openmm\r\n",
        "!conda install -y -c omnia pdbfixer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHUwUcPqI1Ks"
      },
      "source": [
        "import sys\r\n",
        "sys.path.insert(0, \"/usr/local/lib/python3.7/site-packages/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BP6M3Uz2wZ1"
      },
      "source": [
        "!pip install --pre deepchem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAj3Fs5e2xzt"
      },
      "source": [
        "import deepchem as dc\r\n",
        "dc.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDwtlk7DrW39"
      },
      "source": [
        "!wget -c http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/lipo.csv\r\n",
        "!wget -c http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/hppb.csv\r\n",
        "!wget -c http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/clearance.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJqZE5TkrcRn"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(123)\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn import svm\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "tf.random.set_seed(123)\r\n",
        "import deepchem as dc\r\n",
        "from deepchem.models.graph_models import GraphConvModel\r\n",
        "\r\n",
        "BATCH_SIZE = 128\r\n",
        "# Set to higher values to get better numbers\r\n",
        "MAX_EPOCH = 50\r\n",
        "LR = 1e-3\r\n",
        "LMBDA = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRUwHe4broQ0"
      },
      "source": [
        "def load_dataset(dataset_file, featurizer='ECFP', split='index'):\r\n",
        "  tasks = ['target']\r\n",
        "\r\n",
        "  if featurizer == 'ECFP':\r\n",
        "    featurizer = dc.feat.CircularFingerprint(size=1024)\r\n",
        "  elif featurizer == 'GraphConv':\r\n",
        "    featurizer = dc.feat.ConvMolFeaturizer()\r\n",
        "\r\n",
        "  loader = dc.data.CSVLoader(\r\n",
        "      tasks=tasks, smiles_field=\"smile\", featurizer=featurizer)\r\n",
        "  dataset = loader.featurize(dataset_file, shard_size=8192)\r\n",
        "\r\n",
        "  transformers = [\r\n",
        "      dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset)\r\n",
        "  ]\r\n",
        "  for transformer in transformers:\r\n",
        "    dataset = transformer.transform(dataset)\r\n",
        "\r\n",
        "  splitters = {\r\n",
        "      'index': dc.splits.IndexSplitter(),\r\n",
        "      'random': dc.splits.RandomSplitter(),\r\n",
        "      'scaffold': dc.splits.ScaffoldSplitter()\r\n",
        "  }\r\n",
        "  splitter = splitters[split]\r\n",
        "  train, valid, test = splitter.train_valid_test_split(dataset)\r\n",
        "  return tasks, (train, valid, test), transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlhS-E7tsFGa"
      },
      "source": [
        "def experiment(dataset_file, method='GraphConv', split='scaffold'):\r\n",
        "  featurizer = 'ECFP'\r\n",
        "  if method == 'GraphConv':\r\n",
        "    featurizer = 'GraphConv'\r\n",
        "  tasks, datasets, transformers = load_dataset(\r\n",
        "      dataset_file, featurizer=featurizer, split=split)\r\n",
        "  train, val, test = datasets\r\n",
        "\r\n",
        "  model = None\r\n",
        "  if method == 'GraphConv':\r\n",
        "    #Opciones\r\n",
        "    #GraphConvModel(n_tasks: int, graph_conv_layers: List[int] = [64, 64], \r\n",
        "    #dense_layer_size: int = 128, dropout: float = 0.0, \r\n",
        "    #mode: str = 'classification', number_atom_features: int = 75, \r\n",
        "    #n_classes: int = 2, batch_size: int = 100, \r\n",
        "    #batch_normalize: bool = True, uncertainty: bool = False)\r\n",
        "\r\n",
        "    #Original\r\n",
        "    model = GraphConvModel(len(tasks), batch_size=BATCH_SIZE, mode=\"regression\")\r\n",
        "\r\n",
        "    #Opcion1\r\n",
        "    #model = GraphConvModel(len(tasks), batch_size=BATCH_SIZE, dropout=0.5, mode=\"regression\")\r\n",
        "\r\n",
        "    #Opcion2\r\n",
        "    #model = GraphConvModel(len(tasks), batch_size=BATCH_SIZE, dropout=0.5, number_atom_features=64, mode=\"regression\")\r\n",
        "\r\n",
        "    #Opcion3\r\n",
        "    #model = GraphConvModel(len(tasks), batch_size=BATCH_SIZE, dropout=0.3, dense_layer_size = 128, mode=\"regression\")\r\n",
        "\r\n",
        "    #Opcion4\r\n",
        "    #model = GraphConvModel(len(tasks), graph_conv_layers=[2048, 1024], batch_size=BATCH_SIZE, dropout=0.3, dense_layer_size = 128, mode=\"regression\")\r\n",
        "\r\n",
        "\r\n",
        "  elif method == 'RF':\r\n",
        "\r\n",
        "    def model_builder_rf(model_dir):\r\n",
        "      sklearn_model = RandomForestRegressor(n_estimators=100)\r\n",
        "      return dc.models.SklearnModel(sklearn_model, model_dir)\r\n",
        "\r\n",
        "    model = dc.models.SingletaskToMultitask(tasks, model_builder_rf)\r\n",
        "  elif method == 'SVR':\r\n",
        "\r\n",
        "    def model_builder_svr(model_dir):\r\n",
        "      sklearn_model = svm.SVR(kernel='linear')\r\n",
        "      return dc.models.SklearnModel(sklearn_model, model_dir)\r\n",
        "\r\n",
        "    model = dc.models.SingletaskToMultitask(tasks, model_builder_svr)\r\n",
        "\r\n",
        "  return model, train, val, test, transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V0o3cOfsGIb"
      },
      "source": [
        "#from keras.utils.vis_utils import plot_model\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\r\n",
        "\r\n",
        "\r\n",
        "def benchmark(  MODEL = \"GraphConv\", SPLIT = \"scaffold\",  DATASET = \"hppb.csv\"):\r\n",
        "\r\n",
        "\r\n",
        "  metric = dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)\r\n",
        "  \r\n",
        "  print(\"About to build model\")\r\n",
        "  model, train, val, test, transformers = experiment(\r\n",
        "      DATASET, method=MODEL, split=SPLIT)\r\n",
        "  #plot_model(model.model,show_shapes=True, show_layer_names=True)\r\n",
        "  \r\n",
        "  if MODEL == 'GraphConv':\r\n",
        "    print(\"running GraphConv search\")\r\n",
        "    best_val_score = 0.0\r\n",
        "    train_score = 0.0\r\n",
        "    for l in range(0, MAX_EPOCH):\r\n",
        "      print(\"epoch %d\" % l, end=\" \")\r\n",
        "      if (l+1) % 10 == 0:\r\n",
        "        print()\r\n",
        "      model.fit(train, nb_epoch=1)\r\n",
        "      latest_train_score = model.evaluate(train, [metric],\r\n",
        "                                          transformers)['mean-pearson_r2_score']\r\n",
        "      latest_val_score = model.evaluate(val, [metric],\r\n",
        "                                        transformers)['mean-pearson_r2_score']\r\n",
        "      if latest_val_score > best_val_score:\r\n",
        "        best_val_score = latest_val_score\r\n",
        "        train_score = latest_train_score\r\n",
        "    print()\r\n",
        "    print((MODEL, SPLIT, DATASET, train_score, best_val_score))\r\n",
        "    print(model.model.summary())\r\n",
        "  else:\r\n",
        "    model.fit(train)\r\n",
        "    train_score = model.evaluate(train, [metric],\r\n",
        "                                 transformers)['mean-pearson_r2_score']\r\n",
        "    val_score = model.evaluate(val, [metric],\r\n",
        "                               transformers)['mean-pearson_r2_score']\r\n",
        "    print()   \r\n",
        "    print((MODEL, SPLIT, DATASET, train_score, val_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01NSdimVuZ47"
      },
      "source": [
        "benchmark()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i0ztN9W0sEI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HELYCQ-py1O0"
      },
      "source": [
        "benchmark(  \"GraphConv\", \"scaffold\",  \"lipo.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66HH-pkB0M_Z"
      },
      "source": [
        "benchmark(\"RF\", \"scaffold\",  \"lipo.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKLNd9DL0QPs"
      },
      "source": [
        "benchmark(  \"SVR\", \"scaffold\",  \"lipo.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdiYEKllaP4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}