{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase03-02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PabloCGarcia/bioDeepLearning/blob/main/Clase02_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66a5-1HvCiyg"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# create a pipeline object\n",
        "pipe = make_pipeline(\n",
        "     StandardScaler(),\n",
        "     LogisticRegression()\n",
        ")\n",
        "\n",
        "# load the iris dataset and split it into train and test sets\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# fit the whole pipeline\n",
        "tmp = pipe.fit(X_train, y_train)\n",
        "print(tmp)\n",
        "# we can now use it like any other estimator\n",
        "accuracy_score(pipe.predict(X_test), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJOL_LboGBvJ"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import randint\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# define the parameter space that will be searched over\n",
        "param_distributions = {'n_estimators': randint(1, 5),\n",
        "                       'max_depth': randint(5, 10)}\n",
        "\n",
        "# now create a searchCV object and fit it to the data\n",
        "search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),\n",
        "                            n_iter=5,\n",
        "                            param_distributions=param_distributions,\n",
        "                            random_state=0)\n",
        "tmp= search.fit(X_train, y_train)\n",
        "print(tmp)\n",
        "print(search.best_params_)\n",
        "\n",
        "# the search object now acts like a normal random forest estimator\n",
        "# with max_depth=9 and n_estimators=4\n",
        "search.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWAZpt65Aa-h"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# different learning rate schedules and momentum parameters\n",
        "params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,\n",
        "           'learning_rate_init': 0.2},\n",
        "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
        "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
        "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
        "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
        "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,\n",
        "           'learning_rate_init': 0.2},\n",
        "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
        "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
        "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
        "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
        "          {'solver': 'adam', 'learning_rate_init': 0.01}]\n",
        "\n",
        "labels = [\"constant learning-rate\", \"constant with momentum\",\n",
        "          \"constant with Nesterov's momentum\",\n",
        "          \"inv-scaling learning-rate\", \"inv-scaling with momentum\",\n",
        "          \"inv-scaling with Nesterov's momentum\", \"adam\"]\n",
        "\n",
        "plot_args = [{'c': 'red', 'linestyle': '-'},\n",
        "             {'c': 'green', 'linestyle': '-'},\n",
        "             {'c': 'blue', 'linestyle': '-'},\n",
        "             {'c': 'red', 'linestyle': '--'},\n",
        "             {'c': 'green', 'linestyle': '--'},\n",
        "             {'c': 'blue', 'linestyle': '--'},\n",
        "             {'c': 'black', 'linestyle': '-'}]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8v4l2aUBLkz"
      },
      "source": [
        "iris = load_iris()\n",
        "\n",
        "name=\"Iris\"\n",
        "print(\"\\nlearning on dataset %s\" % name)\n",
        "\n",
        "X = MinMaxScaler().fit_transform(iris.data)\n",
        "mlps = []\n",
        "max_iter = 20\n",
        "\n",
        "for label, param in zip(labels, params):\n",
        "    print(\"training: %s\" % label)\n",
        "    mlp = MLPClassifier(verbose=0, random_state=0,\n",
        "                        max_iter=max_iter, **param)\n",
        "    mlp.fit(X, iris.target)\n",
        "    mlps.append(mlp)\n",
        "    print(\"Training set score: %f\" % mlp.score(X, iris.target))\n",
        "    print(\"Training set loss: %f\" % mlp.loss_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofTDFSTRE-sr"
      },
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
        "for mlp, label, args in zip(mlps, labels, plot_args):\n",
        "        ax.plot(mlp.loss_curve_, label=label, **args)\n",
        "\n",
        "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiQhazJdQcHQ"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM0pmxY7S1e2"
      },
      "source": [
        "data = pd.DataFrame(data=iris.data, columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\n",
        " \n",
        "# Append class / label data\n",
        "data[\"class\"] = iris.target\n",
        "\n",
        "a=sns.pairplot(data,hue='class',palette=\"muted\",size=5,vars=['sepal_width','sepal_length','petal_length','petal_width'],kind='scatter')\n",
        "\n",
        "#to change the size of scatterpoints\n",
        "a=a.map_offdiag(plt.scatter,s=35,alpha=0.9)\n",
        "\n",
        "#remove the top and the right lines\n",
        "sns.despine()\n",
        "\n",
        "#additional line to adjust some appearance issues\n",
        "plt.subplots_adjust(top=0.9)\n",
        "\n",
        "#set the title of the graph\n",
        "a.fig.suptitle('Relation between Sepal Width and Sepal Length',fontsize=20,color='b',alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljU1bO0MBM7t"
      },
      "source": [
        "digits = datasets.load_digits()\n",
        "\n",
        "name=\"digits\"\n",
        "print(\"\\nlearning on dataset %s\" % name)\n",
        "X = MinMaxScaler().fit_transform(digits.data)\n",
        "mlps = []\n",
        "max_iter = 15\n",
        "\n",
        "for label, param in zip(labels, params):\n",
        "    print(\"training: %s\" % label)\n",
        "    mlp = MLPClassifier(verbose=0, random_state=0,\n",
        "                        max_iter=max_iter, **param)\n",
        "    mlp.fit(X, digits.target)\n",
        "    mlps.append(mlp)\n",
        "    print(\"Training set score: %f\" % mlp.score(X, digits.target))\n",
        "    print(\"Training set loss: %f\" % mlp.loss_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4yKWJQBFJb5"
      },
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
        "for mlp, label, args in zip(mlps, labels, plot_args):\n",
        "        ax.plot(mlp.loss_curve_, label=label, **args)\n",
        "\n",
        "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n0nhPHjxL7P"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mnist = fetch_openml(\"mnist_784\")\n",
        "# rescale the data, use the traditional train/test split\n",
        "X, y = mnist.data / 255., mnist.target\n",
        "X_train, X_test = X[:60000], X[60000:]\n",
        "y_train, y_test = y[:60000], y[60000:]\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
        "                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
        "#mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
        "#                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
        "#                    learning_rate_init=.1)\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
        "\n",
        "fig, axes = plt.subplots(4, 4)\n",
        "# use global min / max to ensure all weights are shown on the same scale\n",
        "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
        "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
        "               vmax=.5 * vmax)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}